{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": true
   },
   "outputs": [],
   "source": [
    "from muselsl import stream, list_muses, view, record\n",
    "from multiprocessing import set_start_method, Process\n",
    "from mne import Epochs, find_events\n",
    "from time import time, strftime, gmtime\n",
    "import os\n",
    "from stimulus_presentation import n170\n",
    "from utils import utils\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "# N170\n",
    "\n",
    "This notebook will run an N170 experiment to analyze event-related potentials indicating the processing of faces.\n",
    "\n",
    "    The N170 was first described by Shlomo Bentin and colleagues in 1996,[5] who measured ERPs from participants viewing faces and other objects. They found that human faces and face parts (such as eyes) elicited different responses than other stimuli, including animal faces, body parts, and cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Connect to an EEG Device\n",
    "*Note: if using Windows 10 and BlueMuse, skip this section and connect using the BlueMuse GUI*\n",
    "\n",
    "Make sure your device is turned on and run the following code. It should detect and connect to the device and begin 'Streaming...'\n",
    "\n",
    "If the device is not found or the connection times out, try running this code again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for available Muse devices\n",
    "muses = list_muses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Start a background process that will stream data from the first available Muse\n",
    "stream_process = Process(target=stream, args=(muses[0]['address'],))\n",
    "stream_process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Apply the EEG Device and Wait for Signal Quality to Stabilize\n",
    "Put the Muse on and run the following code to view the raw EEG data stream.\n",
    "\n",
    "The numbers on the side of the graph indicate the variance of the signal. Wait until this decreases below 10 for all electrodes before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Looking for an EEG stream...\n",
      "Start acquiring data.\n",
      "<muselsl.viewer_v1.LSLViewer object at 0x7f4cbb957198>\n",
      "\n",
      "                toggle filter : d\n",
      "                toogle full screen : f\n",
      "                zoom out : /\n",
      "                zoom in : *\n",
      "                increase time scale : -\n",
      "                decrease time scale : +\n",
      "               \n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Run the Experiment\n",
    "Modify the variables in the following code chunk to define how long you want to run the experiment and the name of the subject and session you are running. Then, seat the subject in front of the computer and have them quietly view the screen until the experiment is completed.\n",
    "\n",
    "Stimuli were presented for 200 ms with an intertrial interval of 400 ms and random jitter of ±100ms. The task was to mentally note whether a “face” or a “house” was presented. Six blocks of 2 min were recorded for a single partipant.\n",
    "\n",
    "The stimuli set consisted in 12 grayscale pictures of centered human faces for the “face” condition, and in 12 grayscale pictures of houses for the “face” condition. The pictures were obtained from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-d58546923f23>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d58546923f23>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    duration =\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Define these parameters \n",
    "duration = \n",
    "subject = \n",
    "session = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "recording_path = os.path.join(os.path.expanduser(\"~\"), \"eeg-notebooks\", \"data\", \"visual\", \"N170\", \"subject\" + subject, \"session\" + session, (\"recording_%s.csv\" %\n",
    "                                              strftime(\"%Y-%m-%d-%H.%M.%S\", gmtime())) + \".csv\")\n",
    "\n",
    "stimulus = Process(target=n170.present, args=(duration,))\n",
    "recording = Process(target=record, args=(duration, recording_path))\n",
    "\n",
    "stimulus.start()\n",
    "recording.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "# Step 4: Clean the Data\n",
    "\n",
    "## Load data into MNE objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = utils.load_data('visual/N170', sfreq=256., \n",
    "                      subject_nb=subject, session_nb=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "raw.plot_psd();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "Filter the data between 1 and 30hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.filter(1,30, method='iir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoching\n",
    "\n",
    "Here we epoch data for -100ms to 800ms after the stimulus. No baseline correction is needed (signal is bandpass filtered) and we reject every epochs were the signal exceed 75 uV. This mainly rejects blinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = find_events(raw)\n",
    "event_id = {'House': 1, 'Face': 2}\n",
    "\n",
    "epochs = Epochs(raw, events=events, event_id=event_id, \n",
    "                tmin=-0.1, tmax=0.8, baseline=None,\n",
    "                reject={'eeg': 100e-6}, preload=True, \n",
    "                verbose=False, picks=[0,1,2,3])\n",
    "print('sample drop %: ', (1 - len(epochs.events)/len(events)) * 100)\n",
    "epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Analyze the Data\n",
    "\n",
    "Now we can analyze our results by averaing the epochs and looking for differences between epochs when faces or houses were presented\n",
    "\n",
    "\n",
    "## Epoch average\n",
    "\n",
    "Now we can plot the average ERP for both conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "conditions = OrderedDict()\n",
    "conditions['House'] = [1]\n",
    "conditions['Face'] = [2]\n",
    "\n",
    "fig, ax = utils.plot_conditions(epochs, conditions=conditions, \n",
    "                                ci=97.5, n_boot=1000, title='',\n",
    "                                diff_waveform=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## What do you see? Describe the shape of the waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it dips down at 0.2 for the faces but not for the houses. It also looks like the faces line goes up above the red line right after 0.2, maybe closer to 0.3. Blah, blah, blah..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write notes about your experiment... What worked? What didn' work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jimmy wasn't sitting still and it made the experiment really noisy, but one of his session was OK and blah blah blah..."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "mne"
  },
  "kernelspec": {
   "display_name": "Python (mne)",
   "language": "python",
   "name": "mne"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nteract": {
   "version": "0.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
